{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "4QczpWHxei-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430a2e3b-f3a4-4a9c-91b2-e1d674ddc5c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel, AutoTokenizer, TrainingArguments, Trainer, DistilBertModel\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import pdb"
      ],
      "metadata": {
        "id": "hJH3sIQYCi_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7419a47-c2ed-414b-b96d-ebdbaa093670"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"sample_data/toyset300.csv\"\n",
        "df = pd.read_csv(path, dtype=\"string\")\n",
        "df['Definition'] = df['Definition'].astype(str)\n",
        "df = df[['Word', 'Definition']]\n",
        "df"
      ],
      "metadata": {
        "id": "fKeOMcbJW5V9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f65bf125-6fc8-4df6-b6d3-e21dcf0e5e62"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Word                                         Definition\n",
              "0         gamecock                 a rooster trained for cockfighting\n",
              "1         gamecock     a fighting cock a rooster used in cockfighting\n",
              "2         gamecock                                 the male game fowl\n",
              "3         gamecock  a cock bred from a fighting stock or strain a ...\n",
              "4         gamecock               someone who is a very fierce fighter\n",
              "...            ...                                                ...\n",
              "998        Pentoic  \"Pertaining to  or desingating an acid (called...\n",
              "999      low-lying  having a small elevation above the ground or h...\n",
              "1000      Extruded                                       \"of Extrude\"\n",
              "1001     high_life                                 excessive spending\n",
              "1002  dance_lesson                                a lesson in dancing\n",
              "\n",
              "[1003 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f222fac-e59f-44f4-898c-6c294870bbd7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Definition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gamecock</td>\n",
              "      <td>a rooster trained for cockfighting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gamecock</td>\n",
              "      <td>a fighting cock a rooster used in cockfighting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gamecock</td>\n",
              "      <td>the male game fowl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gamecock</td>\n",
              "      <td>a cock bred from a fighting stock or strain a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gamecock</td>\n",
              "      <td>someone who is a very fierce fighter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Pentoic</td>\n",
              "      <td>\"Pertaining to  or desingating an acid (called...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>low-lying</td>\n",
              "      <td>having a small elevation above the ground or h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>Extruded</td>\n",
              "      <td>\"of Extrude\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>high_life</td>\n",
              "      <td>excessive spending</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>dance_lesson</td>\n",
              "      <td>a lesson in dancing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1003 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f222fac-e59f-44f4-898c-6c294870bbd7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f222fac-e59f-44f4-898c-6c294870bbd7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f222fac-e59f-44f4-898c-6c294870bbd7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Convert classes to numbers'''\n",
        "word_dict = {} \n",
        "i = 0\n",
        "for w in df['Word'].unique():\n",
        "    word_dict[w] = i\n",
        "    i += 1\n",
        "\n",
        "'''Convert numbers back to words'''\n",
        "idx2word = {v:k for k,v in word_dict.items()}"
      ],
      "metadata": {
        "id": "VbCHKP3OCjFC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df[['Definition','Word']], test_size=0.2,random_state=45)\n",
        "df_test, df_val = train_test_split(df_test[['Definition','Word']], test_size=0.5,random_state=45)"
      ],
      "metadata": {
        "id": "EGzMn3PdCjHp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_enc = tokenizer(df_train['Definition'].to_list(), padding=True, truncation=True, max_length=128)\n",
        "test_enc = tokenizer(df_test['Definition'].to_list(), padding=True, truncation=True, max_length=128)\n",
        "val_enc = tokenizer(df_val['Definition'].to_list(), padding=True, truncation=True, max_length=128)"
      ],
      "metadata": {
        "id": "gbMr5wDgDm_A"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RevDictDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.labels = self.labels.to_list()\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "GQmzYiEwDsIa"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''One hot encoding of classes'''\n",
        "train_label_enum = {k:j+1 for j, k in enumerate(df_train['Word'].unique())}\n",
        "train_label_enum[\"<unk>\"] = 0\n",
        "train_num_labels = len(train_label_enum)\n",
        "idx2token = {idx: token for token, idx in train_label_enum.items()}\n",
        "df_train['labels'] = df_train['Word'].apply(lambda x: [1.0 if train_label_enum[x]==i else 0.0 for i in range(train_num_labels)])\n",
        "# labels = []\n",
        "# for word in list(df_val['Word']):\n",
        "#   if word in train_label_enum:\n",
        "#     labels.append(train_label_enum[word])\n",
        "#   else:\n",
        "#     labels.append(train_label_enum[\"<unk>\"])\n",
        "\n",
        "df_val['labels'] = df_val['Word'].apply(lambda x: [1.0 if train_label_enum[x]==i else 0.0 for i in range(train_num_labels)] if x in train_label_enum else [1.0] + [0.0]*(train_num_labels-1))"
      ],
      "metadata": {
        "id": "Lyi61L9tDsLe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_train_label_enum= {v: k for k, v in train_label_enum.items()}"
      ],
      "metadata": {
        "id": "VNrE9Nf1ZAtt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = RevDictDataset(train_enc, df_train['labels'])\n",
        "val_dataset = RevDictDataset(val_enc, df_val['labels'])"
      ],
      "metadata": {
        "id": "M2vWjuHaDsQu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "class BLmodel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4):\n",
        "        super(BLmodel, self).__init__()\n",
        "        self.bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.lstm_layer_1 = nn.LSTM(input_size=768, hidden_size=hidden_dim1, num_layers=1, batch_first=True)\n",
        "        self.lstm_layer_2 = nn.LSTM(input_size=hidden_dim1, hidden_size=hidden_dim2, num_layers=1, batch_first=True)\n",
        "        self.lstm_layer_3 = nn.LSTM(input_size=hidden_dim2, hidden_size=hidden_dim3, num_layers=1, batch_first=True)\n",
        "        self.lstm_layer_4 = nn.LSTM(input_size=hidden_dim3, hidden_size=hidden_dim4, num_layers=1, batch_first=True)\n",
        "        self.output_layer = nn.Linear(hidden_dim4, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        bert_embedding = outputs[0]\n",
        "        lstm_output_1, _ = self.lstm_layer_1(bert_embedding)\n",
        "        lstm_output_2, _ = self.lstm_layer_2(lstm_output_1)\n",
        "        lstm_output_3, _ = self.lstm_layer_3(lstm_output_2)\n",
        "        lstm_output_4, _ = self.lstm_layer_4(lstm_output_3)\n",
        "        output = self.output_layer(lstm_output_4[:, -1, :])\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "fT1GkN7aO3sl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, train_loader, val_loader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    min_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        # Training\n",
        "        for batch in tqdm(train_loader):\n",
        "            input_ids = batch['input_ids']\n",
        "            labels = batch['labels']\n",
        "            attention_mask= batch['attention_mask']\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids,attention_mask)\n",
        "            # pdb.set_trace()\n",
        "            loss = nn.MSELoss()(outputs.view(-1, len(train_label_enum)), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            total_val_loss = 0.0\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids']\n",
        "                labels = batch['labels']\n",
        "                attention_mask= batch['attention_mask']\n",
        "                outputs = model(input_ids,attention_mask)\n",
        "\n",
        "                # pdb.set_trace()\n",
        "                \n",
        "                # Generate top-k words for validation\n",
        "                # _, topk_indices = torch.topk(outputs, k=10, dim=1)\n",
        "                # for i,idx_row in enumerate(topk_indices):\n",
        "                #   row_words = [inv_train_label_enum[idx.item()] for idx in idx_row]\n",
        "                #   print(f\"Top-10 words for {inv_train_label_enum[labels[i].item()]} generated are: {row_words}\")\n",
        "                \n",
        "                val_loss = nn.MSELoss()(outputs.view(-1, len(train_label_enum)), labels)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        if avg_val_loss < min_val_loss:\n",
        "          min_val_loss = avg_val_loss\n",
        "          torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\"\n",
        "              f\"\\tTrain Loss: {avg_loss:.4f}\"\n",
        "              f\"\\tVal Loss: {avg_val_loss:.4f}\"\n",
        "              f\"\\tMin Val Loss: {min_val_loss:.4f}\")\n",
        "\n",
        "        model.train()\n"
      ],
      "metadata": {
        "id": "EGn23k1nRUow"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the DataLoader for train and validation datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # No need to shuffle for validation\n",
        "\n",
        "# Define the training parameters\n",
        "vocab_size = len(train_label_enum)\n",
        "embedding_dim = 768\n",
        "hidden_dim1 = 256\n",
        "hidden_dim2 = 128\n",
        "hidden_dim3 = 64\n",
        "hidden_dim4 = 32\n",
        "model = BLmodel(vocab_size, embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "num_epochs = 3\n",
        "\n",
        "# Train and validate the model\n",
        "train(model, train_loader, val_loader, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "DTgEXbIjbM2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff609bb-ce5b-46d1-f774-807abb776756"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [06:00<00:00,  3.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1]\tTrain Loss: 0.0111\tVal Loss: 0.0085\tMin Val Loss: 0.0085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for word in list(df_test['Word']):\n",
        "  if word in train_label_enum:\n",
        "    labels.append(train_label_enum[word])\n",
        "  else:\n",
        "    labels.append(train_label_enum[\"<unk>\"])\n",
        "df_test['labels']=labels\n",
        "test_dataset = RevDictDataset(test_enc, df_test['labels'])\n",
        "test_loader = DataLoader(test_dataset, batch_size=8,shuffle=False) "
      ],
      "metadata": {
        "id": "7FFlAPRAlsMc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(train_label_enum)\n",
        "embedding_dim = 768\n",
        "hidden_dim1 = 256\n",
        "hidden_dim2 = 128\n",
        "hidden_dim3 = 64\n",
        "hidden_dim4 = 32\n",
        "model = BLmodel(vocab_size, embedding_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4)\n",
        "model.load_state_dict(torch.load('./model.pt'))\n",
        "\n",
        "#On Validation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  val_final_output=[]\n",
        "  for batch in val_loader:\n",
        "      input_ids = batch['input_ids']\n",
        "      attention_mask= batch['attention_mask']\n",
        "      outputs = model(input_ids,attention_mask)\n",
        "\n",
        "      # Generate top-k words for each instance in the batch\n",
        "      _, topk_indices = torch.topk(outputs, k=100, dim=1)\n",
        "      for i, idx_row in enumerate(topk_indices):\n",
        "          row_words = [inv_train_label_enum[idx.item()] for idx in idx_row]\n",
        "          val_final_output.append(row_words)\n",
        "\n",
        "#On Test\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  test_final_output=[]\n",
        "  for batch in test_loader:\n",
        "      input_ids = batch['input_ids']\n",
        "      attention_mask= batch['attention_mask']\n",
        "      outputs = model(input_ids,attention_mask)\n",
        "\n",
        "      # Generate top-k words for each instance in the batch\n",
        "      _, topk_indices = torch.topk(outputs, k=100, dim=1)\n",
        "      for i, idx_row in enumerate(topk_indices):\n",
        "          row_words = [inv_train_label_enum[idx.item()] for idx in idx_row]\n",
        "          test_final_output.append(row_words)"
      ],
      "metadata": {
        "id": "QNs_dcPcEChs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfaf6bc0-a129-4d9e-b242-51f948dffce7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,word in enumerate(df_val['Word']):\n",
        "  print(f\"Top-100 words generated for {word} are: {val_final_output[i]}\")\n",
        "  print()\n",
        "\n",
        "for i,word in enumerate(df_test['Word']):\n",
        "  print(f\"Top-100 words generated for {word} are: {test_final_output[i]}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "jEEkKxd3Z94B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_one=0\n",
        "top_ten=0\n",
        "top_hundred=0\n",
        "total_words= len(df_val['Word'])\n",
        "for i,word in enumerate(df_val['Word']):\n",
        "  if val_final_output[i][0] == word:\n",
        "    top_one+=1\n",
        "    top_ten+=1\n",
        "    top_hundred+=1\n",
        "  elif word in val_final_output[i][:10]:\n",
        "    top_ten+=1\n",
        "    top_hundred+=1\n",
        "  elif word in val_final_output[i][:100]:\n",
        "    top_hundred+=1\n",
        "\n",
        "print(\"Accuracy for Validation Dataset:\")\n",
        "print(\"Top-1 Accuracy: {:.2f}\".format(top_one/total_words))\n",
        "print(\"Top-10 Accuracy: {:.2f}\".format(top_ten/total_words))\n",
        "print(\"Top-100 Accuracy: {:.2f}\".format(top_hundred/total_words))"
      ],
      "metadata": {
        "id": "On46Q6nZKfN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16faf53-aa8d-4360-f2b0-d304d1d626a2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Validation Dataset:\n",
            "Top-1 Accuracy: 0.00\n",
            "Top-10 Accuracy: 0.04\n",
            "Top-100 Accuracy: 0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_one=0\n",
        "top_ten=0\n",
        "top_hundred=0\n",
        "total_words= len(df_test['Word'])\n",
        "for i,word in enumerate(df_test['Word']):\n",
        "  if test_final_output[i][0] == word:\n",
        "    top_one+=1\n",
        "    top_ten+=1\n",
        "    top_hundred+=1\n",
        "  elif word in test_final_output[i][:10]:\n",
        "    top_ten+=1\n",
        "    top_hundred+=1\n",
        "  elif word in test_final_output[i][:100]:\n",
        "    top_hundred+=1\n",
        "\n",
        "print(\"Accuracy for Test Dataset:\")\n",
        "print(\"Top-1 Accuracy: {:.2f}\".format(top_one/total_words))\n",
        "print(\"Top-10 Accuracy: {:.2f}\".format(top_ten/total_words))\n",
        "print(\"Top-100 Accuracy: {:.2f}\".format(top_hundred/total_words))"
      ],
      "metadata": {
        "id": "SbR58_oAECm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6a65d4-93c5-46b2-e6f5-d6264ae2034c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Test Dataset:\n",
            "Top-1 Accuracy: 0.00\n",
            "Top-10 Accuracy: 0.01\n",
            "Top-100 Accuracy: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXiDaKLXECqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
} 
